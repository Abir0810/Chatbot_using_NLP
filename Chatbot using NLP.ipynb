{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7915e1e3",
   "metadata": {},
   "source": [
    "# Libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be78807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb4686",
   "metadata": {},
   "source": [
    "# NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a73b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e5766",
   "metadata": {},
   "source": [
    "# File read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a794a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\AI/sample.txt','r', encoding='utf8', errors ='ignore') as fin:\n",
    "    raw = fin.read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5870ba6",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a353c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "066657cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c1640",
   "metadata": {},
   "source": [
    "# Greetings Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff5bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    \"\"\"If user's input is a greeting, return a greeting response\"\"\"\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48597a68",
   "metadata": {},
   "source": [
    "# Response Greetings Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04316d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    friday_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        friday_response=friday_response+\"I am sorry! I don't understand you\"\n",
    "        return friday_response\n",
    "    else:\n",
    "        friday_response = friday_response+sent_tokens[idx]\n",
    "        return friday_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be929c1",
   "metadata": {},
   "source": [
    "# Chat with BOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1316e081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friday: Hello This is Friday. How May I help you? If do not need anything, type Bye!\n",
      "Hello\n",
      "Friday: I am glad! You are talking to me\n",
      "Hi\n",
      "Friday: hi there\n",
      "What is Data science? \n",
      "Friday: data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data, and apply knowledge from data across a broad range of application domains.\n",
      "What have you been built by?\n",
      "Friday: i'm built with nlp and python\n",
      "What is Natural Language processing\n",
      "Friday: natural language processing is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "Tell me some greatest football players name\n",
      "Friday: here is some greatest football player name is messi,ronaldo,maradona,pele,ronaldinho,zidane.\n",
      "Messi\n",
      "Friday: messi is one of the greatest footballer all time.he is playing for psg and argentina.\n",
      "Are you free now?\n",
      "Friday: i am always free for you.\n",
      "Are you sad?\n",
      "Friday: i'm sad sometimes.\n",
      "What is AI?\n",
      "Friday: ai is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind.\n",
      "What is your nickname?\n",
      "Friday: my nickname is friday.\n",
      "Who is Abir Abdullha\n",
      "Friday: this friday chatbot system is made by abir abdullha.i working with him.his keen interest in machine learning, data science and nlp.he is working at i-sharify ltd. as project manager.\n",
      "bye\n",
      "Friday: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"Friday: Hello This is Friday. How May I help you? If do not need anything, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"Friday: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"Friday: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"Friday: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"Friday: Bye! take care..\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39020492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
